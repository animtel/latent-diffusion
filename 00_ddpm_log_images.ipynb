{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import lovely_tensors as lt\n",
    "except:\n",
    "    ! pip install --upgrade lovely-tensors\n",
    "    import lovely_tensors as lt\n",
    "    \n",
    "lt.monkey_patch()\n",
    "\n",
    "from pathlib import Path\n",
    "Path.ls = lambda x: list(x.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldm_cfg_path = Path('models/ldm/cin256/config.yaml')\n",
    "ldm_cfg_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "from hydra.utils import instantiate\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "with initialize(version_base=None, config_path=str(ldm_cfg_path.parent)):\n",
    "    cfg = compose(config_name='config')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ldm.util import instantiate_from_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 394.98 M params.\n",
      "Keeping EMAs of 628.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Restored from /app/notebooks/03_ldm_vae/latent-diffusion/logs/2023-05-18T12-05-30_config/checkpoints/last.ckpt with 0 missing and 50 unexpected keys\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = instantiate_from_config(cfg.model)\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': tensor[4, 256, 256, 48] n=12582912 (48Mb) x∈[5.960e-08, 1.000] μ=0.500 σ=0.289 cuda:0,\n",
       " 'class_label': tensor[4] i64 x∈[42, 42] μ=42.000 σ=0. cuda:0 [42, 42, 42, 42],\n",
       " 'human_label': ['054656345', '054656345', '054656345', '054656345']}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = {\n",
    "    \"image\": torch.rand(4, 256, 256, 48).to(device),\n",
    "    \"class_label\": torch.tensor([42, 42, 42, 42]).to(device),\n",
    "    \"human_label\": ['054656345', '054656345', '054656345', '054656345']\n",
    "}\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting: Switched to EMA weights\n",
      "Data shape for DDIM sampling is (4, 4, 32, 32), eta 1.0\n",
      "Running DDIM Sampling with 2 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 2/2 [00:00<00:00, 18.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting: Restored training weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting Quantized Denoised: Switched to EMA weights\n",
      "Data shape for DDIM sampling is (4, 4, 32, 32), eta 1.0\n",
      "Running DDIM Sampling with 2 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 2/2 [00:00<00:00, 17.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting Quantized Denoised: Restored training weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting Inpaint: Switched to EMA weights\n",
      "Data shape for DDIM sampling is (4, 4, 32, 32), eta 1.0\n",
      "Running DDIM Sampling with 2 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 2/2 [00:00<00:00, 18.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting Inpaint: Restored training weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting Outpaint: Switched to EMA weights\n",
      "Data shape for DDIM sampling is (4, 4, 32, 32), eta 1.0\n",
      "Running DDIM Sampling with 2 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 2/2 [00:00<00:00, 17.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting Outpaint: Restored training weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting Progressives: Switched to EMA weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progressive Generation: 100%|██████████| 1000/1000 [00:54<00:00, 18.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting Progressives: Restored training weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progressive Generation: 100%|██████████| 6/6 [00:00<00:00, 17.98it/s]\n"
     ]
    }
   ],
   "source": [
    "log_res = model.log_images(x, ddim_steps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b_triplane2rgb(x, mult=20):\n",
    "    feat_size = x.shape[1] // 3\n",
    "    xy_plane = x[:, :feat_size].mean(dim=1, keepdim=True).repeat(1, 3, 1, 1)\n",
    "    xz_plane = x[:, feat_size:feat_size*2].mean(dim=1, keepdim=True).repeat(1, 3, 1, 1)\n",
    "    yz_plane = x[:, feat_size*2:].mean(dim=1, keepdim=True).repeat(1, 3, 1, 1)\n",
    "\n",
    "    triplane = torch.cat([xy_plane, xz_plane, yz_plane], dim=-1)*mult\n",
    "    return triplane\n",
    "\n",
    "def triplane2rgb(x, mult=20):\n",
    "    feat_size = x.shape[0] // 3\n",
    "    xy_plane = x[:feat_size].mean(dim=0, keepdim=True).repeat(1, 3, 1, 1)\n",
    "    # xz_plane = x[feat_size:feat_size*2].mean(dim=1, keepdim=True).repeat(1, 3, 1, 1)\n",
    "    # yz_plane = x[feat_size*2:].mean(dim=1, keepdim=True).repeat(1, 3, 1, 1)\n",
    "\n",
    "    triplane = torch.cat([xy_plane], dim=-1)*mult\n",
    "    return triplane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in log_res.items():\n",
    "    if val.shape[1] == 48:\n",
    "        log_res[key] = b_triplane2rgb(val)\n",
    "    if val.shape[0] == 48:\n",
    "        log_res[key] = triplane2rgb(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputs': tensor[4, 3, 256, 768] n=2359296 (9Mb) x∈[3.575, 16.759] μ=9.999 σ=1.443 cuda:0,\n",
       " 'reconstruction': tensor[4, 3, 256, 768] n=2359296 (9Mb) x∈[0.112, 0.309] μ=0.195 σ=0.013 cuda:0,\n",
       " 'conditioning': tensor[4, 3, 256, 256] f64 n=786432 (6Mb) x∈[-1.000, 1.000] μ=0.996 σ=0.080,\n",
       " 'diffusion_row': tensor[1, 3, 1034, 1550] n=4808100 (18Mb) x∈[-0.050, 0.473] μ=0.188 σ=0.055 cuda:0,\n",
       " 'samples': tensor[4, 3, 256, 768] n=2359296 (9Mb) x∈[-0.195, 0.477] μ=0.186 σ=0.065 cuda:0,\n",
       " 'samples_x0_quantized': tensor[4, 3, 256, 768] n=2359296 (9Mb) x∈[-0.141, 0.473] μ=0.181 σ=0.063 cuda:0,\n",
       " 'samples_inpainting': tensor[4, 3, 256, 768] n=2359296 (9Mb) x∈[-0.111, 0.398] μ=0.186 σ=0.034 cuda:0,\n",
       " 'mask': tensor[4, 1, 32, 32] n=4096 (16Kb) x∈[0., 1.000] μ=0.750 σ=0.433 cuda:0,\n",
       " 'samples_outpainting': tensor[4, 3, 256, 768] n=2359296 (9Mb) x∈[-0.176, 0.400] μ=0.186 σ=0.033 cuda:0,\n",
       " 'progressive_row': tensor[1, 3, 1034, 1550] n=4808100 (18Mb) x∈[-0.052, 0.464] μ=0.195 σ=0.061 cuda:0}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputs': tensor[4, 48, 256, 256] n=12582912 (48Mb) x∈[5.960e-08, 1.000] μ=0.500 σ=0.289 cuda:0,\n",
       " 'reconstruction': tensor[4, 48, 256, 256] n=12582912 (48Mb) x∈[-0.005, 0.029] μ=0.010 σ=0.004 cuda:0,\n",
       " 'conditioning': tensor[4, 3, 256, 256] f64 n=786432 (6Mb) x∈[-1.000, 1.000] μ=0.996 σ=0.080,\n",
       " 'diffusion_row': tensor[48, 1034, 1550] n=76929600 (0.3Gb) x∈[-0.063, 0.092] μ=0.009 σ=0.006 cuda:0,\n",
       " 'samples': tensor[4, 48, 256, 256] n=12582912 (48Mb) x∈[-0.058, 0.076] μ=0.009 σ=0.006 cuda:0,\n",
       " 'samples_x0_quantized': tensor[4, 48, 256, 256] n=12582912 (48Mb) x∈[-0.062, 0.084] μ=0.009 σ=0.006 cuda:0,\n",
       " 'samples_inpainting': tensor[4, 48, 256, 256] n=12582912 (48Mb) x∈[-0.047, 0.056] μ=0.009 σ=0.005 cuda:0,\n",
       " 'mask': tensor[4, 1, 32, 32] n=4096 (16Kb) x∈[0., 1.000] μ=0.750 σ=0.433 cuda:0,\n",
       " 'samples_outpainting': tensor[4, 48, 256, 256] n=12582912 (48Mb) x∈[-0.046, 0.056] μ=0.009 σ=0.005 cuda:0,\n",
       " 'progressive_row': tensor[48, 1034, 1550] n=76929600 (0.3Gb) x∈[-0.063, 0.092] μ=0.009 σ=0.006 cuda:0}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_res.items()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
